{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","editable":false,"execution":{"iopub.execute_input":"2024-01-08T11:22:57.374700Z","iopub.status.busy":"2024-01-08T11:22:57.374131Z","iopub.status.idle":"2024-01-08T11:23:15.555959Z","shell.execute_reply":"2024-01-08T11:23:15.554997Z","shell.execute_reply.started":"2024-01-08T11:22:57.374669Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train and validation sets with labels for airplane are created.\n","Train and validation sets with labels for airport are created.\n","Train and validation sets with labels for bare soil are created.\n","Train and validation sets with labels for baseball diamond are created.\n","Train and validation sets with labels for basketball court are created.\n","Train and validation sets with labels for beach are created.\n","Train and validation sets with labels for bridge are created.\n","Train and validation sets with labels for buildings are created.\n","Train and validation sets with labels for cars are created.\n","Train and validation sets with labels for chaparral are created.\n","Train and validation sets with labels for cloud are created.\n","Train and validation sets with labels for containers are created.\n","Train and validation sets with labels for crosswalk are created.\n","Train and validation sets with labels for dense residential area are created.\n","Train and validation sets with labels for desert are created.\n","Train and validation sets with labels for dock are created.\n","Train and validation sets with labels for factory are created.\n","Train and validation sets with labels for field are created.\n","Train and validation sets with labels for football field are created.\n","Train and validation sets with labels for forest are created.\n","Train and validation sets with labels for freeway are created.\n","Train and validation sets with labels for golf course are created.\n","Train and validation sets with labels for grass are created.\n","Train and validation sets with labels for greenhouse are created.\n","Train and validation sets with labels for gully are created.\n","Train and validation sets with labels for habor are created.\n","Train and validation sets with labels for intersection are created.\n","Train and validation sets with labels for island are created.\n","Train and validation sets with labels for lake are created.\n","Train and validation sets with labels for mobile home are created.\n","Train and validation sets with labels for mountain are created.\n","Train and validation sets with labels for overpass are created.\n","Train and validation sets with labels for park are created.\n","Train and validation sets with labels for parking lot are created.\n","Train and validation sets with labels for parkway are created.\n","Train and validation sets with labels for pavement are created.\n","Train and validation sets with labels for railway are created.\n","Train and validation sets with labels for railway station are created.\n","Train and validation sets with labels for river are created.\n","Train and validation sets with labels for road are created.\n","Train and validation sets with labels for roundabout are created.\n","Train and validation sets with labels for runway are created.\n","Train and validation sets with labels for sand are created.\n","Train and validation sets with labels for sea are created.\n","Train and validation sets with labels for ships are created.\n","Train and validation sets with labels for snow are created.\n","Train and validation sets with labels for snowberg are created.\n","Train and validation sets with labels for sparse residential area are created.\n","Train and validation sets with labels for stadium are created.\n","Train and validation sets with labels for swimming pool are created.\n","Train and validation sets with labels for tanks are created.\n","Train and validation sets with labels for tennis court are created.\n","Train and validation sets with labels for terrace are created.\n","Train and validation sets with labels for track are created.\n","Train and validation sets with labels for trail are created.\n","Train and validation sets with labels for transmission tower are created.\n","Train and validation sets with labels for trees are created.\n","Train and validation sets with labels for water are created.\n","Train and validation sets with labels for wetland are created.\n","Train and validation sets with labels for wind turbine are created.\n","DataLoaders for training and validation are created for airplane\n","DataLoaders for training and validation are created for airport\n","DataLoaders for training and validation are created for bare soil\n","DataLoaders for training and validation are created for baseball diamond\n","DataLoaders for training and validation are created for basketball court\n","DataLoaders for training and validation are created for beach\n","DataLoaders for training and validation are created for bridge\n","DataLoaders for training and validation are created for buildings\n","DataLoaders for training and validation are created for cars\n","DataLoaders for training and validation are created for chaparral\n","DataLoaders for training and validation are created for cloud\n","DataLoaders for training and validation are created for containers\n","DataLoaders for training and validation are created for crosswalk\n","DataLoaders for training and validation are created for dense residential area\n","DataLoaders for training and validation are created for desert\n","DataLoaders for training and validation are created for dock\n","DataLoaders for training and validation are created for factory\n","DataLoaders for training and validation are created for field\n","DataLoaders for training and validation are created for football field\n","DataLoaders for training and validation are created for forest\n","DataLoaders for training and validation are created for freeway\n","DataLoaders for training and validation are created for golf course\n","DataLoaders for training and validation are created for grass\n","DataLoaders for training and validation are created for greenhouse\n","DataLoaders for training and validation are created for gully\n","DataLoaders for training and validation are created for habor\n","DataLoaders for training and validation are created for intersection\n","DataLoaders for training and validation are created for island\n","DataLoaders for training and validation are created for lake\n","DataLoaders for training and validation are created for mobile home\n","DataLoaders for training and validation are created for mountain\n","DataLoaders for training and validation are created for overpass\n","DataLoaders for training and validation are created for park\n","DataLoaders for training and validation are created for parking lot\n","DataLoaders for training and validation are created for parkway\n","DataLoaders for training and validation are created for pavement\n","DataLoaders for training and validation are created for railway\n","DataLoaders for training and validation are created for railway station\n","DataLoaders for training and validation are created for river\n","DataLoaders for training and validation are created for road\n","DataLoaders for training and validation are created for roundabout\n","DataLoaders for training and validation are created for runway\n","DataLoaders for training and validation are created for sand\n","DataLoaders for training and validation are created for sea\n","DataLoaders for training and validation are created for ships\n","DataLoaders for training and validation are created for snow\n","DataLoaders for training and validation are created for snowberg\n","DataLoaders for training and validation are created for sparse residential area\n","DataLoaders for training and validation are created for stadium\n","DataLoaders for training and validation are created for swimming pool\n","DataLoaders for training and validation are created for tanks\n","DataLoaders for training and validation are created for tennis court\n","DataLoaders for training and validation are created for terrace\n","DataLoaders for training and validation are created for track\n","DataLoaders for training and validation are created for trail\n","DataLoaders for training and validation are created for transmission tower\n","DataLoaders for training and validation are created for trees\n","DataLoaders for training and validation are created for water\n","DataLoaders for training and validation are created for wetland\n","DataLoaders for training and validation are created for wind turbine\n"]}],"source":["#============================Import Lib============================\n","\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","import cv2\n","import matplotlib.pyplot as plt\n","# from tqdm.auto import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torchvision.models as models\n","from sklearn.model_selection import train_test_split\n","\n","import warnings\n","warnings.filterwarnings(action='ignore') \n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","#============================HyperParam Setting============================\n","CFG = {\n","    'IMG_SIZE':224,\n","    'EPOCHS':3,\n","    'LEARNING_RATE':3e-4,\n","    'BATCH_SIZE':64,\n","    'SEED':626\n","}\n","# 시드 고정 \n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(CFG['SEED']) # Seed 고정\n","#============================TRAIN DATA LOAD============================\n","#============================라벨별로 잘라서 저장(\"라벨이름_data.csv\")============================\n","train = pd.read_csv('/kaggle/input/dacon00/train.csv')\n","label_columns = train.columns[2:]\n","save_directory = '/kaggle/working/'\n","for label in label_columns:\n","    # 현재 라벨에 해당하는 데이터만 필터링\n","    filtered_df = train[label]\n","    filtered_df = pd.concat([train.iloc[:, 0:2], filtered_df], axis=1)\n","    # 필터링된 데이터를 별도의 파일로 저장\n","    # 파일 경로를 포함하여 저장\n","    filtered_df.to_csv(os.path.join(save_directory, f'train_{label}.csv'), index=False)\n","# water_data.csv\n","#============================TEST DATA LOAD============================\n","#============================라벨별로 잘라서 저장(\"라벨이름_data.csv\")============================\n","test = pd.read_csv('/kaggle/input/dacon00/test.csv')\n","# water_data.csv\n","#============================\"라벨이름_data.csv\" 불러오기============================\n","# 라벨 데이터가 있는 폴더 경로\n","folder_path = '/kaggle/working/'\n","# 모든 CSV 파일의 리스트를 가져옴\n","file_list = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n","\n","# 각 파일에 대해 반복\n","def rewrite(df):\n","    df['img_path'] = '/kaggle/input/dacon00/' + df['img_path']\n","    return df\n","def get_labels(df):\n","    return df.iloc[:,2:].values\n","for label in label_columns:\n","    # CSV 파일 경로\n","    file_path = os.path.join(folder_path, f'train_{label}.csv')\n","\n","    # CSV 파일 읽기\n","    globals()[label] = pd.read_csv(file_path)\n","\n","    # 이미지 경로 수정\n","    globals()[label] = rewrite(globals()[label])\n","\n","    # 훈련 데이터와 검증 데이터로 분할\n","    train_df, val_df = train_test_split(globals()[label], train_size=0.8, random_state=42)\n","\n","    # 분할된 데이터셋을 변수에 저장\n","    globals()[f\"train_{label}\"] = train_df\n","    globals()[f\"val_{label}\"] = val_df\n","\n","    # 훈련 및 검증 데이터셋에서 라벨 추출\n","    globals()[f\"train_{label}_labels\"] = get_labels(train_df)\n","    globals()[f\"val_{label}_labels\"] = get_labels(val_df)\n","\n","    print(f\"Train and validation sets with labels for {label} are created.\")\n","    \n","train_water    \n","val_water\n","train_water_labels\n","val_water_labels\n","\n","#============================Data Load2============================\n","class CustomDataset(Dataset):\n","    def __init__(self, img_path_list, label_list, transform=None):\n","        self.img_path_list = img_path_list\n","        self.label_list = label_list\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        img_path = self.img_path_list[index]\n","\n","        # PIL 이미지로 불러오기\n","        image = Image.open(img_path).convert(\"RGB\")\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        \n","        if self.label_list is not None:\n","            label = torch.tensor(self.label_list[index], dtype=torch.float32)\n","            return image, label\n","        else:\n","            return image\n","        \n","    def __len__(self):\n","        return len(self.img_path_list)\n","\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])), #,interpolation=transforms.InterpolationMode.LANCZOS\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","#============================TRAIN/VALID============================\n","train_loaders = {}\n","val_loaders = {}\n","\n","for label in label_columns:\n","    # 훈련 데이터셋 생성\n","    train_dataset = CustomDataset(\n","        img_path_list=globals()[f\"train_{label}\"]['img_path'].tolist(),\n","        label_list=globals()[f\"train_{label}_labels\"],\n","        transform=train_transform\n","    )\n","\n","    # 검증 데이터셋 생성\n","    val_dataset = CustomDataset(\n","        img_path_list=globals()[f\"val_{label}\"]['img_path'].tolist(),\n","        label_list=globals()[f\"val_{label}_labels\"],\n","        transform=test_transform\n","    )\n","\n","    # 훈련 DataLoader 생성\n","    train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n","    train_loaders[label] = train_loader\n","\n","    # 검증 DataLoader 생성\n","    val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n","    val_loaders[label] = val_loader\n","\n","    print(f\"DataLoaders for training and validation are created for {label}\")\n","train_loaders['trees']\n","val_loaders['trees']\n","    \n","#============================Modeling(ResNet101)============================\n","class BaseModel(nn.Module):\n","    def __init__(self, num_classes=1):\n","        super(BaseModel, self).__init__()\n","        self.backbone = models.resnet101(pretrained=True)\n","        self.classifier = nn.Linear(1000, num_classes)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = F.sigmoid(self.classifier(x))\n","        return x\n","#============================Training============================\n","def train(model, optimizer, train_loader, val_loader, device):\n","    model.to(device)\n","    criterion = nn.BCELoss().to(device)\n","\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for epoch in range(1, CFG['EPOCHS']+1):\n","        model.train()\n","        train_loss = []\n","        for imgs, labels in tqdm(iter(train_loader)):\n","            imgs = imgs.float().to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            output = model(imgs)\n","            loss = criterion(output, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss.append(loss.item())\n","\n","        _val_loss = validation(model, criterion, val_loader, device)\n","        _train_loss = np.mean(train_loss)\n","        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}]')\n","\n","        if best_val_loss > _val_loss:\n","            best_val_loss = _val_loss\n","            best_model = model\n","\n","    return best_model\n","\n","def validation(model, criterion, val_loader, device):\n","    model.eval()\n","    val_loss = []\n","    with torch.no_grad():\n","        for imgs, labels in tqdm(iter(val_loader)):\n","            imgs = imgs.float().to(device)\n","            labels = labels.to(device)\n","\n","            probs = model(imgs)\n","\n","            loss = criterion(probs, labels)\n","\n","            val_loss.append(loss.item())\n","\n","        _val_loss = np.mean(val_loss)\n","\n","    return _val_loss\n","\n","##########################\n","# 'tanks', 'tennis court', 'terrace', 'track', 'trail',\n","# 'transmission tower', 'trees', 'water', 'wetland', 'wind turbine'\n"]},{"cell_type":"code","execution_count":4,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-01-09T02:39:27.378912Z","iopub.status.busy":"2024-01-09T02:39:27.378041Z","iopub.status.idle":"2024-01-09T02:39:42.313451Z","shell.execute_reply":"2024-01-09T02:39:42.312062Z","shell.execute_reply.started":"2024-01-09T02:39:27.378859Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65ffe1e05a294b858e67b2ada197aaaf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/819 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m label_columns\n\u001b[0;32m----> 7\u001b[0m infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtanks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtanks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 학습 진행\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(model, test_loader, device):\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n","Cell \u001b[0;32mIn[1], line 227\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    225\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    226\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28miter\u001b[39m(train_loader)):\n\u001b[1;32m    228\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    229\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    141\u001b[0m image\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(img_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_COLOR)\n\u001b[1;32m    143\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m--> 145\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTER_LANCZOS4\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# image=cv2.medianBlur(image,3)\u001b[39;00m\n\u001b[1;32m    148\u001b[0m image\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mfilter2D(image,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_sharpening)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["model = BaseModel()\n","num_gpu = torch.cuda.device_count()  # Dual GPU in KAGGLE\n","if num_gpu > 1:\n","    model = nn.DataParallel(model)\n","optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n","label_columns\n","infer_model = train(model, optimizer, train_loaders['tennis court'], val_loaders['tennis court'], device) # 학습 진행\n","\n","def inference(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for imgs in tqdm(iter(test_loader)):\n","            imgs = imgs.float().to(device)\n","            \n","            probs = model(imgs)\n","\n","            probs  = probs.cpu().detach().numpy()\n","            predictions += probs.tolist()\n","    return predictions\n","\n","preds = inference(infer_model, test_loader, device)\n","\n","submit = sample_submission\n","submit.iloc[:,1:] = preds\n","\n","submit.to_csv(\"C:/Users/tkdgn/Desktop/open/tennis court.csv\", index = False) "]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["model = BaseModel()\n","num_gpu = torch.cuda.device_count()  # Dual GPU in KAGGLE\n","if num_gpu > 1:\n","    model = nn.DataParallel(model)\n","optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n","label_columns\n","infer_model = train(model, optimizer, train_loaders['terrace'], val_loaders['terrace'], device) # 학습 진행\n","\n","\n","def inference(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for imgs in tqdm(iter(test_loader)):\n","            imgs = imgs.float().to(device)\n","            \n","            probs = model(imgs)\n","\n","            probs  = probs.cpu().detach().numpy()\n","            predictions += probs.tolist()\n","    return predictions\n","\n","preds = inference(infer_model, test_loader, device)\n","\n","submit = sample_submission\n","submit.iloc[:,1:] = preds\n","\n","submit.to_csv(\"C:/Users/tkdgn/Desktop/open/terrace.csv\", index = False) \n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# **#########LABEL 34REAL FIT##############**"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# **######자아 돌려보자고~~######**** :: LABEL 34"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# #######REALLLLLLLL!!!!!!!!!!!!!!!"]},{"cell_type":"code","execution_count":3,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-01-09T02:48:55.827213Z","iopub.status.busy":"2024-01-09T02:48:55.826805Z","iopub.status.idle":"2024-01-09T02:49:13.673145Z","shell.execute_reply":"2024-01-09T02:49:13.672201Z","shell.execute_reply.started":"2024-01-09T02:48:55.827179Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train and validation sets with labels for airplane are created.\n","Train and validation sets with labels for airport are created.\n","Train and validation sets with labels for bare soil are created.\n","Train and validation sets with labels for baseball diamond are created.\n","Train and validation sets with labels for basketball court are created.\n","Train and validation sets with labels for beach are created.\n","Train and validation sets with labels for bridge are created.\n","Train and validation sets with labels for buildings are created.\n","Train and validation sets with labels for cars are created.\n","Train and validation sets with labels for chaparral are created.\n","Train and validation sets with labels for cloud are created.\n","Train and validation sets with labels for containers are created.\n","Train and validation sets with labels for crosswalk are created.\n","Train and validation sets with labels for dense residential area are created.\n","Train and validation sets with labels for desert are created.\n","Train and validation sets with labels for dock are created.\n","Train and validation sets with labels for factory are created.\n","Train and validation sets with labels for field are created.\n","Train and validation sets with labels for football field are created.\n","Train and validation sets with labels for forest are created.\n","Train and validation sets with labels for freeway are created.\n","Train and validation sets with labels for golf course are created.\n","Train and validation sets with labels for grass are created.\n","Train and validation sets with labels for greenhouse are created.\n","Train and validation sets with labels for gully are created.\n","Train and validation sets with labels for habor are created.\n","Train and validation sets with labels for intersection are created.\n","Train and validation sets with labels for island are created.\n","Train and validation sets with labels for lake are created.\n","Train and validation sets with labels for mobile home are created.\n","Train and validation sets with labels for mountain are created.\n","Train and validation sets with labels for overpass are created.\n","Train and validation sets with labels for park are created.\n","Train and validation sets with labels for parking lot are created.\n","Train and validation sets with labels for parkway are created.\n","Train and validation sets with labels for pavement are created.\n","Train and validation sets with labels for railway are created.\n","Train and validation sets with labels for railway station are created.\n","Train and validation sets with labels for river are created.\n","Train and validation sets with labels for road are created.\n","Train and validation sets with labels for roundabout are created.\n","Train and validation sets with labels for runway are created.\n","Train and validation sets with labels for sand are created.\n","Train and validation sets with labels for sea are created.\n","Train and validation sets with labels for ships are created.\n","Train and validation sets with labels for snow are created.\n","Train and validation sets with labels for snowberg are created.\n","Train and validation sets with labels for sparse residential area are created.\n","Train and validation sets with labels for stadium are created.\n","Train and validation sets with labels for swimming pool are created.\n","Train and validation sets with labels for tanks are created.\n","Train and validation sets with labels for tennis court are created.\n","Train and validation sets with labels for terrace are created.\n","Train and validation sets with labels for track are created.\n","Train and validation sets with labels for trail are created.\n","Train and validation sets with labels for transmission tower are created.\n","Train and validation sets with labels for trees are created.\n","Train and validation sets with labels for water are created.\n","Train and validation sets with labels for wetland are created.\n","Train and validation sets with labels for wind turbine are created.\n","DataLoaders for training and validation are created for airplane\n","DataLoaders for training and validation are created for airport\n","DataLoaders for training and validation are created for bare soil\n","DataLoaders for training and validation are created for baseball diamond\n","DataLoaders for training and validation are created for basketball court\n","DataLoaders for training and validation are created for beach\n","DataLoaders for training and validation are created for bridge\n","DataLoaders for training and validation are created for buildings\n","DataLoaders for training and validation are created for cars\n","DataLoaders for training and validation are created for chaparral\n","DataLoaders for training and validation are created for cloud\n","DataLoaders for training and validation are created for containers\n","DataLoaders for training and validation are created for crosswalk\n","DataLoaders for training and validation are created for dense residential area\n","DataLoaders for training and validation are created for desert\n","DataLoaders for training and validation are created for dock\n","DataLoaders for training and validation are created for factory\n","DataLoaders for training and validation are created for field\n","DataLoaders for training and validation are created for football field\n","DataLoaders for training and validation are created for forest\n","DataLoaders for training and validation are created for freeway\n","DataLoaders for training and validation are created for golf course\n","DataLoaders for training and validation are created for grass\n","DataLoaders for training and validation are created for greenhouse\n","DataLoaders for training and validation are created for gully\n","DataLoaders for training and validation are created for habor\n","DataLoaders for training and validation are created for intersection\n","DataLoaders for training and validation are created for island\n","DataLoaders for training and validation are created for lake\n","DataLoaders for training and validation are created for mobile home\n","DataLoaders for training and validation are created for mountain\n","DataLoaders for training and validation are created for overpass\n","DataLoaders for training and validation are created for park\n","DataLoaders for training and validation are created for parking lot\n","DataLoaders for training and validation are created for parkway\n","DataLoaders for training and validation are created for pavement\n","DataLoaders for training and validation are created for railway\n","DataLoaders for training and validation are created for railway station\n","DataLoaders for training and validation are created for river\n","DataLoaders for training and validation are created for road\n","DataLoaders for training and validation are created for roundabout\n","DataLoaders for training and validation are created for runway\n","DataLoaders for training and validation are created for sand\n","DataLoaders for training and validation are created for sea\n","DataLoaders for training and validation are created for ships\n","DataLoaders for training and validation are created for snow\n","DataLoaders for training and validation are created for snowberg\n","DataLoaders for training and validation are created for sparse residential area\n","DataLoaders for training and validation are created for stadium\n","DataLoaders for training and validation are created for swimming pool\n","DataLoaders for training and validation are created for tanks\n","DataLoaders for training and validation are created for tennis court\n","DataLoaders for training and validation are created for terrace\n","DataLoaders for training and validation are created for track\n","DataLoaders for training and validation are created for trail\n","DataLoaders for training and validation are created for transmission tower\n","DataLoaders for training and validation are created for trees\n","DataLoaders for training and validation are created for water\n","DataLoaders for training and validation are created for wetland\n","DataLoaders for training and validation are created for wind turbine\n","READY\n"]}],"source":["#============================Import Lib============================\n","import random\n","import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","import cv2\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import torchvision.models as models\n","from sklearn.model_selection import train_test_split\n","\n","\n","import warnings\n","warnings.filterwarnings(action='ignore') \n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","#============================HyperParam Setting============================\n","CFG = {\n","    'IMG_SIZE':224,\n","    'EPOCHS':3,\n","    'LEARNING_RATE':3e-4,\n","    'BATCH_SIZE':64,\n","    'SEED':41\n","}\n","# 시드 고정 \n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(CFG['SEED']) # Seed 고정\n","#============================TRAIN DATA LOAD============================\n","#============================라벨별로 잘라서 저장(\"라벨이름_data.csv\")============================\n","train = pd.read_csv('/kaggle/input/dacon00/train.csv')\n","label_columns = train.columns[2:]\n","save_directory = '/kaggle/working/'\n","for label in label_columns:\n","    # 현재 라벨에 해당하는 데이터만 필터링\n","    filtered_df = train[label]\n","    filtered_df = pd.concat([train.iloc[:, 0:2], filtered_df], axis=1)\n","    # 필터링된 데이터를 별도의 파일로 저장\n","    # 파일 경로를 포함하여 저장\n","    filtered_df.to_csv(os.path.join(save_directory, f'train_{label}.csv'), index=False)\n","# water_data.csv\n","#============================TEST DATA LOAD============================\n","#============================라벨별로 잘라서 저장(\"라벨이름_data.csv\")============================\n","test = pd.read_csv('/kaggle/input/dacon00/test.csv')\n","# water_data.csv\n","#============================\"라벨이름_data.csv\" 불러오기============================\n","# 라벨 데이터가 있는 폴더 경로\n","folder_path = '/kaggle/working/'\n","# 모든 CSV 파일의 리스트를 가져옴\n","file_list = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n","\n","# 각 파일에 대해 반복\n","def rewrite(df):\n","    df['img_path'] = '/kaggle/input/dacon00/' + df['img_path']\n","    return df\n","def get_labels(df):\n","    return df.iloc[:,2:].values\n","for label in label_columns:\n","    # CSV 파일 경로\n","    file_path = os.path.join(folder_path, f'train_{label}.csv')\n","\n","    # CSV 파일 읽기\n","    globals()[label] = pd.read_csv(file_path)\n","\n","    # 이미지 경로 수정\n","    globals()[label] = rewrite(globals()[label])\n","\n","    # 훈련 데이터와 검증 데이터로 분할\n","    train_df, val_df = train_test_split(globals()[label], train_size=0.8, random_state=42)\n","\n","    # 분할된 데이터셋을 변수에 저장\n","    globals()[f\"train_{label}\"] = train_df\n","    globals()[f\"val_{label}\"] = val_df\n","\n","    # 훈련 및 검증 데이터셋에서 라벨 추출\n","    globals()[f\"train_{label}_labels\"] = get_labels(train_df)\n","    globals()[f\"val_{label}_labels\"] = get_labels(val_df)\n","\n","    print(f\"Train and validation sets with labels for {label} are created.\")\n","dfs=[train_water, test]\n","test = [rewrite(df) for df in dfs] \n","test = test[1]\n","train_water    \n","val_water\n","train_water_labels\n","val_water_labels\n","\n","#============================Data Load2============================\n","\n","class CustomDataset():\n","    def __init__(self, img_path_list, label_list, aug_mode='normal', mode='train'):\n","        self.img_path_list=img_path_list\n","        self.label_list=label_list\n","        self.aug_mode=aug_mode\n","        self.mode=mode\n","        \n","        self.kernel_sharpening=np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n","        \n","        if self.aug_mode=='normal' or self.mode=='test':\n","            self.transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            ])\n","            \n","        if self.aug_mode=='rotate':\n","            self.transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n","                transforms.RandomRotation([-180, 180]),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            ])\n","            \n","        if self.aug_mode=='flip':\n","            self.transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n","                transforms.RandomHorizontalFlip(p=1),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","            ])\n","            \n","        \n","        \n","    def __getitem__(self, index):\n","            img_path=self.img_path_list[index]\n","            \n","            image=cv2.imread(img_path, cv2.IMREAD_COLOR)\n","            \n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            \n","            image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n","\n","            # image=cv2.medianBlur(image,3)\n","            image=cv2.filter2D(image,-1, self.kernel_sharpening)\n","            \n","            \n","            if self.transform is not None:\n","                image=self.transform(image)\n","                \n","            if self.label_list is not None:\n","                label = torch.tensor(self.label_list[index], dtype=torch.float32)\n","                return image,label\n","            else:\n","                return image\n","            \n","    def __len__(self):\n","        return len(self.img_path_list)   \n","\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE']),interpolation=transforms.InterpolationMode.BICUBIC),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE']),interpolation=transforms.InterpolationMode.BICUBIC),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","#============================TRAIN/VALID============================\n","train_loaders = {}\n","val_loaders = {}\n","\n","for label in label_columns:\n","    # 훈련 데이터셋 생성\n","    train_dataset = CustomDataset(\n","        img_path_list=globals()[f\"train_{label}\"]['img_path'].tolist(),\n","        label_list=globals()[f\"train_{label}_labels\"]\n","        \n","    )\n","\n","    # 검증 데이터셋 생성\n","    val_dataset = CustomDataset(\n","        img_path_list=globals()[f\"val_{label}\"]['img_path'].tolist(),\n","        label_list=globals()[f\"val_{label}_labels\"]\n","       \n","    )\n","\n","    # 훈련 DataLoader 생성\n","    train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n","    train_loaders[label] = train_loader\n","\n","    # 검증 DataLoader 생성\n","    val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n","    val_loaders[label] = val_loader\n","\n","    print(f\"DataLoaders for training and validation are created for {label}\")\n","train_loaders['trees']\n","val_loaders['trees']\n","    \n","#============================Modeling(ResNet101)============================\n","class BaseModel(nn.Module):\n","    def __init__(self, num_classes=1):\n","        super(BaseModel, self).__init__()\n","        self.backbone = models.resnet101(pretrained=True)\n","        self.classifier = nn.Linear(1000, num_classes)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = F.sigmoid(self.classifier(x))\n","        return x\n","#============================Training============================\n","def train(model, optimizer, train_loader, val_loader, device):\n","    model.to(device)\n","    criterion = nn.BCELoss().to(device)\n","\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for epoch in range(1, CFG['EPOCHS']+1):\n","        model.train()\n","        train_loss = []\n","        for imgs, labels in tqdm(iter(train_loader)):\n","            imgs = imgs.float().to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            output = model(imgs)\n","            loss = criterion(output, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss.append(loss.item())\n","\n","        _val_loss = validation(model, criterion, val_loader, device)\n","        _train_loss = np.mean(train_loss)\n","        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}]')\n","\n","        if best_val_loss > _val_loss:\n","            best_val_loss = _val_loss\n","            best_model = model\n","\n","    return best_model\n","\n","def validation(model, criterion, val_loader, device):\n","    model.eval()\n","    val_loss = []\n","    with torch.no_grad():\n","        for imgs, labels in tqdm(iter(val_loader)):\n","            imgs = imgs.float().to(device)\n","            labels = labels.to(device)\n","\n","            probs = model(imgs)\n","\n","            loss = criterion(probs, labels)\n","\n","            val_loss.append(loss.item())\n","\n","        _val_loss = np.mean(val_loss)\n","\n","    return _val_loss\n","\n","torch.cuda.empty_cache()\n","##=====================training================================##\n","##=====================training================================##\n","##=====================training================================##\n","##=====================training================================##\n","##=====================training================================##\n","##=====================training================================##\n","\n","Label1 = label_columns[0:10]\n","Label2 = label_columns[10:20]\n","Label3 = label_columns[20:30]\n","Label4 = label_columns[30:40]\n","Label5 = label_columns[40:50]\n","Label6 = label_columns[50:60]\n","print(\"READY\")\n","best_models = {}\n","\n","def inference(model, test_loader, device):\n","    model.to(device)\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for imgs in tqdm(iter(test_loader)):\n","            imgs = imgs.float().to(device)\n","            \n","            probs = model(imgs)\n","\n","            probs  = probs.cpu().detach().numpy()\n","            predictions += probs.tolist()\n","    return predictions"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# TRIANING START"]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# LABEL 3"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-01-09T02:49:30.996589Z","iopub.status.busy":"2024-01-09T02:49:30.996208Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training model for label: freeway\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [19:24<00:00,  1.42s/it]\n","100%|██████████| 205/205 [03:18<00:00,  1.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.04190] Val Loss : [0.02449]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:27<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:55<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.02071] Val Loss : [0.01796]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:34<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:58<00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.01627] Val Loss : [0.01403]\n","Training model for label: golf course\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:27<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:52<00:00,  1.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.04858] Val Loss : [0.03763]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:01<00:00,  1.05it/s]\n","100%|██████████| 205/205 [01:56<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.02536] Val Loss : [0.01476]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:23<00:00,  1.02it/s]\n","100%|██████████| 205/205 [01:53<00:00,  1.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.01589] Val Loss : [0.01992]\n","Training model for label: grass\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:29<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:55<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.45457] Val Loss : [0.38754]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:23<00:00,  1.02it/s]\n","100%|██████████| 205/205 [01:52<00:00,  1.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.37215] Val Loss : [0.39298]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:22<00:00,  1.02it/s]\n","100%|██████████| 205/205 [01:58<00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.34038] Val Loss : [0.36917]\n","Training model for label: greenhouse\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:56<00:00,  1.02s/it]\n","100%|██████████| 205/205 [01:59<00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.07461] Val Loss : [0.04668]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:45<00:00,  1.01s/it]\n","100%|██████████| 205/205 [01:56<00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.05010] Val Loss : [0.03947]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:43<00:00,  1.01s/it]\n","100%|██████████| 205/205 [01:57<00:00,  1.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.03510] Val Loss : [0.03043]\n","Training model for label: gully\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:33<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:56<00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.11119] Val Loss : [0.07214]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:36<00:00,  1.00it/s]\n","100%|██████████| 205/205 [01:55<00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.07261] Val Loss : [0.06530]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:29<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:56<00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.06171] Val Loss : [0.07230]\n","Training model for label: habor\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:43<00:00,  1.01s/it]\n","100%|██████████| 205/205 [01:58<00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.07832] Val Loss : [0.05140]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:26<00:00,  1.02it/s]\n","100%|██████████| 205/205 [01:58<00:00,  1.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.06706] Val Loss : [0.06540]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:21<00:00,  1.02it/s]\n","100%|██████████| 205/205 [01:48<00:00,  1.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.05628] Val Loss : [0.04531]\n","Training model for label: intersection\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:13<00:00,  1.03it/s]\n","100%|██████████| 205/205 [01:59<00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.07040] Val Loss : [0.04040]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:39<00:00,  1.00s/it]\n","100%|██████████| 205/205 [01:58<00:00,  1.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.03959] Val Loss : [0.03916]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:37<00:00,  1.00it/s]\n","100%|██████████| 205/205 [01:53<00:00,  1.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.02836] Val Loss : [0.03303]\n","Training model for label: island\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:12<00:00,  1.03it/s]\n","100%|██████████| 205/205 [01:42<00:00,  2.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.06511] Val Loss : [0.41218]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:22<00:00,  1.02it/s]\n","100%|██████████| 205/205 [02:00<00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.03741] Val Loss : [0.03794]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:30<00:00,  1.01it/s]\n","100%|██████████| 205/205 [01:50<00:00,  1.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.03682] Val Loss : [0.02821]\n","Training model for label: lake\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:02<00:00,  1.05it/s]\n","100%|██████████| 205/205 [01:39<00:00,  2.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1], Train Loss : [0.08770] Val Loss : [0.05354]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [12:53<00:00,  1.06it/s]\n","100%|██████████| 205/205 [01:41<00:00,  2.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.04539] Val Loss : [0.03482]\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [12:56<00:00,  1.05it/s]\n","100%|██████████| 205/205 [01:48<00:00,  1.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3], Train Loss : [0.03458] Val Loss : [0.03318]\n","Training model for label: mobile home\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 819/819 [13:11<00:00,  1.04it/s]\n","100%|██████████| 819/819 [13:10<00:00,  1.04it/s]\n","100%|██████████| 205/205 [01:51<00:00,  1.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2], Train Loss : [0.07573] Val Loss : [0.09740]\n"]},{"name":"stderr","output_type":"stream","text":[" 59%|█████▉    | 487/819 [07:50<05:18,  1.04it/s]"]}],"source":["\n","for label in Label3:\n","    print(f\"Training model for label: {label}\")\n","\n","    # 각 라벨에 대한 훈련 및 검증 데이터 로더\n","    train_loader = train_loaders[label]\n","    val_loader = val_loaders[label]\n","\n","    # 새 모델 인스턴스 생성\n","    model = BaseModel()\n","    num_gpu = torch.cuda.device_count()  # Dual GPU in KAGGLE\n","    if num_gpu > 1:\n","        model = nn.DataParallel(model)\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n","\n","    # 모델 훈련\n","    trained_model = train(model, optimizer, train_loader, val_loader, device)\n","\n","    # 최적의 모델 저장\n","    best_models[label] = trained_model\n","\n","    # 모델 저장 (옵션)\n","    torch.save(trained_model.state_dict(), f'/kaggle/working/best_model_{label}.pth')\n","\n","test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n","\n","def load_model(label):\n","    model = BaseModel()\n","    if num_gpu > 1:\n","        model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load(f\"/kaggle/working/best_model_{label}.pth\"))\n","    model.to(device)\n","    return model\n","\n","for label in Label3:\n","    print(f\"Predicting for label: {label}\")\n","\n","    # 테스트 데이터 로더 설정\n","    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n","    # 저장된 모델 불러오기\n","    model = load_model(label)\n","\n","    # 추론 실행\n","    preds = inference(model, test_loader, device)\n","\n","    # 추론 결과 저장\n","    sample_submission = pd.read_csv('/kaggle/input/dacon00/sample_submission.csv')\n","    sample_submission[label] = preds\n","    sample_submission.to_csv(f\"/kaggle/working/{label}.csv\", index=False)\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Label3=Label3[10]\n","for label in Label3:\n","    print(f\"Training model for label: {label}\")\n","\n","    # 각 라벨에 대한 훈련 및 검증 데이터 로더\n","    train_loader = train_loaders[label]\n","    val_loader = val_loaders[label]\n","\n","    # 새 모델 인스턴스 생성\n","    model = BaseModel()\n","    num_gpu = torch.cuda.device_count()  # Dual GPU in KAGGLE\n","    if num_gpu > 1:\n","        model = nn.DataParallel(model)\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n","\n","    # 모델 훈련\n","    trained_model = train(model, optimizer, train_loader, val_loader, device)\n","\n","    # 최적의 모델 저장\n","    best_models[label] = trained_model\n","\n","    # 모델 저장 (옵션)\n","    torch.save(trained_model.state_dict(), f'/kaggle/working/best_model_{label}.pth')\n","\n","test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n","\n","def load_model(label):\n","    model = BaseModel()\n","    if num_gpu > 1:\n","        model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load(f\"/kaggle/working/best_model_{label}.pth\"))\n","    model.to(device)\n","    return model\n","\n","for label in Label3:\n","    print(f\"Predicting for label: {label}\")\n","\n","    # 테스트 데이터 로더 설정\n","    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n","    # 저장된 모델 불러오기\n","    model = load_model(label)\n","\n","    # 추론 실행\n","    preds = inference(model, test_loader, device)\n","\n","    # 추론 결과 저장\n","    sample_submission = pd.read_csv('/kaggle/input/dacon00/sample_submission.csv')\n","    sample_submission[label] = preds\n","    sample_submission.to_csv(f\"/kaggle/working/{label}.csv\", index=False)\n","    \n","    "]},{"cell_type":"markdown","metadata":{"editable":false},"source":["# LABEL 4"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false},"outputs":[],"source":["\n","##=====================training================================##\n","##=====================training================================##\n","##=====================training================================##\n","\n","for label in Label4:\n","    print(f\"Training model for label: {label}\")\n","\n","    # 각 라벨에 대한 훈련 및 검증 데이터 로더\n","    train_loader = train_loaders[label]\n","    val_loader = val_loaders[label]\n","\n","    # 새 모델 인스턴스 생성\n","    model = BaseModel()\n","    num_gpu = torch.cuda.device_count()  # Dual GPU in KAGGLE\n","    if num_gpu > 1:\n","        model = nn.DataParallel(model)\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=CFG[\"LEARNING_RATE\"])\n","\n","    # 모델 훈련\n","    trained_model = train(model, optimizer, train_loader, val_loader, device)\n","\n","    # 최적의 모델 저장\n","    best_models[label] = trained_model\n","\n","    # 모델 저장 (옵션)\n","    torch.save(trained_model.state_dict(), f'/kaggle/working/best_model_{label}.pth')\n","    \n","def load_model(label):\n","    model = BaseModel()\n","    if num_gpu > 1:\n","        model = nn.DataParallel(model)\n","    model.load_state_dict(torch.load(f\"/kaggle/working/best_model_{label}.pth\"))\n","    model.to(device)\n","    return model\n","\n","for label in Label4:\n","    print(f\"Predicting for label: {label}\")\n","\n","    # 테스트 데이터 로더 설정\n","    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n","    # 저장된 모델 불러오기\n","    model = load_model(label)\n","\n","    # 추론 실행\n","    preds = inference(model, test_loader, device)\n","\n","    # 추론 결과 저장\n","    sample_submission = pd.read_csv('/kaggle/input/vision/sample_submission.csv')\n","    sample_submission[label] = preds\n","    sample_submission.to_csv(f\"/kaggle/working/{label}.csv\", index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4276425,"sourceId":7361926,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
